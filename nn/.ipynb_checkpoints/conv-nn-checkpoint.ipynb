{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "display_step = 10\n",
    "\n",
    "# To prevent overfitting\n",
    "dropout = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset and iterator creation\n",
    "\n",
    "in_dir = \"../input/preprocessed-normalized/\"\n",
    "test_normal_dir = in_dir + \"test/NORMAL\"\n",
    "test_pneumonia_dir = in_dir + \"test/PNEUMONIA\"\n",
    "train_normal_dir = in_dir + \"train/NORMAL\"\n",
    "train_pneumonia_dir = in_dir + \"train/PNEUMONIA\"\n",
    "val_normal_dir = in_dir + \"val/NORMAL\"\n",
    "val_pneumonia_dir = in_dir + \"val/PNEUMONIA\"\n",
    "\n",
    "full_url = np.vectorize(lambda url,prev_url: prev_url+\"/\"+url)\n",
    "test_normal_data = pd.DataFrame(full_url(np.array(os.listdir(test_normal_dir)),test_normal_dir), columns=[\"image_dir\"])\n",
    "test_pneumonia_data = pd.DataFrame(full_url(np.array(os.listdir(test_pneumonia_dir)),test_pneumonia_dir), columns=[\"image_dir\"])\n",
    "train_normal_data = pd.DataFrame(full_url(np.array(os.listdir(train_normal_dir)),train_normal_dir), columns=[\"image_dir\"])\n",
    "train_pneumonia_data = pd.DataFrame(full_url(np.array(os.listdir(train_pneumonia_dir)),train_pneumonia_dir), columns=[\"image_dir\"])\n",
    "val_normal_data = pd.DataFrame(full_url(np.array(os.listdir(val_normal_dir)),val_normal_dir), columns=[\"image_dir\"])\n",
    "val_pneumonia_data = pd.DataFrame(full_url(np.array(os.listdir(val_pneumonia_dir)),val_pneumonia_dir), columns=[\"image_dir\"])\n",
    "\n",
    "test_normal_data[\"class\"] = \"NORMAL\"\n",
    "test_pneumonia_data[\"class\"] = \"PNEUNOMIA\"\n",
    "train_normal_data[\"class\"] = \"NORMAL\"\n",
    "train_pneumonia_data[\"class\"] = \"PNEUNOMIA\"\n",
    "val_normal_data[\"class\"] = \"NORMAL\"\n",
    "val_pneumonia_data[\"class\"] = \"PNEUNOMIA\"\n",
    "\n",
    "test_data = test_normal_data.append(test_pneumonia_data)\n",
    "train_data = train_normal_data.append(train_pneumonia_data)\n",
    "val_data = val_normal_data.append(val_pneumonia_data)\n",
    "\n",
    "# Total ammount of landmarks\n",
    "n_classes = 2\n",
    "\n",
    "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "# to a fixed shape.\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string)\n",
    "    image_decoded.set_shape((256, 256, 1))\n",
    "    return image_decoded, label\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    train_data = tf.data.Dataset.from_tensor_slices(\n",
    "        (train_data[\"image_dir\"].values, \n",
    "         pd.get_dummies(train_data[\"class\"])[\"PNEUNOMIA\"].values))\n",
    "    train_data = train_data.shuffle(buffer_size=10000)\n",
    "\n",
    "    # for a small batch size\n",
    "    train_data = train_data.map(_parse_function, num_parallel_calls=4)\n",
    "    train_data = train_data.batch(batch_size)\n",
    "\n",
    "    # for a large batch size (hundreds or thousands)\n",
    "    # dataset = dataset.apply(tf.contrib.data.map_and_batch(\n",
    "    #    map_func=_parse_function, batch_size=batch_size))\n",
    "\n",
    "    # with gpu usage\n",
    "    train_data = train_data.prefetch(1)\n",
    "    \n",
    "    test_data = tf.data.Dataset.from_tensor_slices(\n",
    "        (test_data[\"image_dir\"].values, \n",
    "         pd.get_dummies(test_data[\"class\"])[\"PNEUNOMIA\"].values))\n",
    "    test_data = test_data.map(_parse_function, num_parallel_calls=4)\n",
    "    test_data = test_data.batch(batch_size)\n",
    "    test_data = test_data.prefetch(1)\n",
    "\n",
    "    val_data = tf.data.Dataset.from_tensor_slices(\n",
    "        (val_data[\"image_dir\"].values, \n",
    "         pd.get_dummies(val_data[\"class\"])[\"PNEUNOMIA\"].values))\n",
    "    val_data = val_data.map(_parse_function, num_parallel_calls=4)\n",
    "    val_data = val_data.batch(1)\n",
    "    val_data = val_data.prefetch(1)\n",
    "    \n",
    "    iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                               train_data.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init = iterator.make_initializer(train_data) # Inicializador para train_data\n",
    "    test_init = iterator.make_initializer(test_data) # Inicializador para test_data\n",
    "    val_init = iterator.make_initializer(val_data) # Inicializador para test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 256, 256, 1])\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, n_classes])\n",
    "\n",
    "def conv2d(img, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add\\\n",
    "        (tf.nn.conv2d(img, w,\\\n",
    "        strides=[1, 1, 1, 1],\\\n",
    "        padding='SAME'),b))\n",
    "\n",
    "def max_pool(img, k):\n",
    "    return tf.nn.max_pool(img, \\\n",
    "        ksize=[1, k, k, 1],\\\n",
    "        strides=[1, k, k, 1],\\\n",
    "        padding='SAME')\n",
    "\n",
    "def avg_pool(img, k):\n",
    "    return tf.nn.avg_pool(img, \\\n",
    "        ksize=[1, k, k, 1],\\\n",
    "        strides=[1, k, k, 1],\\\n",
    "        padding='SAME')\n",
    "\n",
    "# weights and bias conv layer 1\n",
    "wc1 = tf.Variable(tf.random_normal([3, 3, 1, 64]))\n",
    "bc1 = tf.Variable(tf.random_normal([64]))\n",
    "\n",
    "# conv layer\n",
    "conv1 = conv2d(x,wc1,bc1)\n",
    "\n",
    "# Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 128*128 matrix.\n",
    "conv1 = max_pool(conv1, k=2)\n",
    "# conv1 = avg_pool(conv1, k=2)\n",
    "\n",
    "# dropout to reduce overfitting\n",
    "keep_prob = tf. placeholder(tf.float32)\n",
    "conv1 = tf.nn.dropout(conv1,keep_prob)\n",
    "\n",
    "# weights and bias conv layer 2\n",
    "wc2 = tf.Variable(tf.random_normal([3, 3, 64, 64]))\n",
    "bc2 = tf.Variable(tf.random_normal([64]))\n",
    "\n",
    "# conv layer\n",
    "conv2 = conv2d(conv1,wc2,bc2)\n",
    "\n",
    "# Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 64*64 matrix.\n",
    "conv2 = max_pool(conv2, k=2)\n",
    "# conv2 = avg_pool(conv2, k=2)\n",
    "\n",
    "# dropout to reduce overfitting\n",
    "conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "\n",
    "# weights and bias conv layer 3\n",
    "wc3 = tf.Variable(tf.random_normal([1, 1, 64, 128]))\n",
    "bc3 = tf.Variable(tf.random_normal([128]))\n",
    "\n",
    "# conv layer\n",
    "conv3 = conv2d(conv2,wc3,bc3)\n",
    "\n",
    "# Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 32*32 matrix.\n",
    "conv3 = max_pool(conv3, k=2)\n",
    "# conv3 = avg_pool(conv3, k=2)\n",
    "\n",
    "# dropout to reduce overfitting\n",
    "conv3 = tf.nn.dropout(conv3, keep_prob)\n",
    "\n",
    "# weights and bias fc 1\n",
    "wd1 = tf.Variable(tf.random_normal([32*32*128, 512]))\n",
    "bd1 = tf.Variable(tf.random_normal([512]))\n",
    "\n",
    "# fc 1\n",
    "dense1 = tf.reshape(conv3, [-1, wd1.get_shape().as_list()[0]])\n",
    "dense1 = tf.nn.relu(tf.add(tf.matmul(dense1, wd1),bd1))\n",
    "dense1 = tf.nn.dropout(dense1, keep_prob)\n",
    "\n",
    "# weights and bias fc 2\n",
    "wd2 = tf.Variable(tf.random_normal([512, 512]))\n",
    "bd2 = tf.Variable(tf.random_normal([512]))\n",
    "\n",
    "# fc 2\n",
    "dense2 = tf.reshape(dense1, [-1, wd2.get_shape().as_list()[0]])\n",
    "dense2 = tf.nn.relu(tf.add(tf.matmul(dense2, wd2),bd2))\n",
    "dense2 = tf.nn.dropout(dense2, keep_prob)\n",
    "\n",
    "# weights and bias out\n",
    "wout = tf.Variable(tf.random_normal([512, n_classes]))\n",
    "bout = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# prediction\n",
    "pred = tf.add(tf.matmul(dense2, wout), bout)\n",
    "\n",
    "# softmax\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    }
   ],
   "source": [
    "# Session start\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Required to get the filename matching to run.\n",
    "    sess.run(init)\n",
    "    \n",
    "    step = 1\n",
    "    # Compute epochs.\n",
    "    for i in range(training_epochs):\n",
    "        print(\"epoch: {}\".format(i))\n",
    "        sess.run(train_init)\n",
    "        try:\n",
    "            while True:\n",
    "                batch_xs, batch_ys = sess.run(next_element)\n",
    "\n",
    "                sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout}) \n",
    "\n",
    "                if step % display_step == 0:\n",
    "                    acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys,  keep_prob: 1.})\n",
    "                    loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "                    print(\"step: {}\".format(step))\n",
    "                    print(\"accuracy: {}\".format(acc))\n",
    "                    print(\"loss: {}\".format(loss))\n",
    "                    print(\"\\n\")\n",
    "                step += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            # Validate\n",
    "\n",
    "            print(\"Validation\\n\")\n",
    "            sess.run(val_init)\n",
    "            avg_acc = 0\n",
    "            avg_loss = 0\n",
    "            steps=0\n",
    "            try:\n",
    "                while True:\n",
    "                    batch_xs, batch_ys = sess.run(next_element)\n",
    "\n",
    "                    acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys,  keep_prob: 1.})\n",
    "                    loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "                    avg_acc += acc\n",
    "                    avg_loss += loss\n",
    "                    steps += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(steps,(avg_acc / steps) * 100))\n",
    "                print(\"Average validation set loss over {} iterations is {:.2f}\".format(steps,(avg_loss / steps)))\n",
    "    # Test            \n",
    "\n",
    "    print(\"Test\\n\")\n",
    "    sess.run(test_init)\n",
    "    avg_acc = 0\n",
    "    avg_loss = 0\n",
    "    steps=0\n",
    "    try:\n",
    "        while True:\n",
    "            batch_xs, batch_ys = sess.run(next_element)\n",
    "\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys,  keep_prob: 1.})\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            avg_acc += acc\n",
    "            avg_loss += loss\n",
    "            steps += 1\n",
    "            print(\"accuracy: {}\".format(acc))\n",
    "            print(\"loss: {}\".format(loss))\n",
    "            print(\"\\n\")\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(steps,(avg_acc / steps) * 100))\n",
    "        print(\"Average validation set loss over {} iterations is {:.2f}\".format(steps,(avg_loss / steps)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
