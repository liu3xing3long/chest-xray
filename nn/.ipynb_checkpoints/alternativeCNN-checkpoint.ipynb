{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import os, random, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "display_step = 10\n",
    "alpha = 0.5\n",
    "\n",
    "# To prevent overfitting\n",
    "dropout = 0.75\n",
    "\n",
    "summaries_dir = \"./logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and iterator creation\n",
    "\n",
    "in_dir = \"../input/preprocessed-normalized/\"\n",
    "test_normal_dir = in_dir + \"test/NORMAL\"\n",
    "test_pneumonia_dir = in_dir + \"test/PNEUMONIA\"\n",
    "train_normal_dir = in_dir + \"train/NORMAL\"\n",
    "train_pneumonia_dir = in_dir + \"train/PNEUMONIA\"\n",
    "\n",
    "full_url = np.vectorize(lambda url,prev_url: prev_url+\"/\"+url)\n",
    "test_normal_data = pd.DataFrame(full_url(np.array(os.listdir(test_normal_dir)),test_normal_dir), columns=[\"image_dir\"])\n",
    "test_pneumonia_data = pd.DataFrame(full_url(np.array(os.listdir(test_pneumonia_dir)),test_pneumonia_dir), columns=[\"image_dir\"])\n",
    "train_normal_data = pd.DataFrame(full_url(np.array(os.listdir(train_normal_dir)),train_normal_dir), columns=[\"image_dir\"])\n",
    "train_pneumonia_data = pd.DataFrame(full_url(np.array(os.listdir(train_pneumonia_dir)),train_pneumonia_dir), columns=[\"image_dir\"])\n",
    "\n",
    "test_normal_data[\"class\"] = \"NORMAL\"\n",
    "test_pneumonia_data[\"class\"] = \"PNEUNOMIA\"\n",
    "train_normal_data[\"class\"] = \"NORMAL\"\n",
    "train_pneumonia_data[\"class\"] = \"PNEUNOMIA\"\n",
    "\n",
    "test_data = test_normal_data.append(test_pneumonia_data)\n",
    "train_data = train_normal_data.append(train_pneumonia_data)\n",
    "\n",
    "shape_sum = test_data.shape[0]+train_data.shape[0]\n",
    "\n",
    "class_weight = tf.constant([\n",
    "    (test_normal_data.shape[0]+train_normal_data.shape[0])/shape_sum,\n",
    "    (test_pneumonia_data.shape[0]+train_pneumonia_data.shape[0])/shape_sum]) \n",
    "\n",
    "# Total ammount of landmarks\n",
    "n_classes = 2\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "\n",
    "    # Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "    # to a fixed shape.\n",
    "    def _parse_function(filename, label):\n",
    "        image_string = tf.read_file(filename)\n",
    "        image_decoded = tf.image.decode_jpeg(image_string)\n",
    "        image_decoded = tf.cast(image_decoded, tf.float32)\n",
    "        image_decoded.set_shape((256, 256, 1))\n",
    "        return image_decoded, label\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices(\n",
    "        (train_data[\"image_dir\"].values, \n",
    "         pd.get_dummies(train_data[\"class\"]).values))\n",
    "    train_data = train_data.shuffle(buffer_size=10000)\n",
    "\n",
    "    # for a small batch size\n",
    "    train_data = train_data.map(_parse_function, num_parallel_calls=4)\n",
    "    train_data = train_data.batch(batch_size)\n",
    "\n",
    "    # for a large batch size (hundreds or thousands)\n",
    "    # dataset = dataset.apply(tf.contrib.data.map_and_batch(\n",
    "    #    map_func=_parse_function, batch_size=batch_size))\n",
    "\n",
    "    # with gpu usage\n",
    "    train_data = train_data.prefetch(1)\n",
    "    \n",
    "    test_data = tf.data.Dataset.from_tensor_slices(\n",
    "        (test_data[\"image_dir\"].values, \n",
    "         pd.get_dummies(test_data[\"class\"]).values))\n",
    "    test_data = test_data.map(_parse_function, num_parallel_calls=4)\n",
    "    test_data = test_data.batch(batch_size)\n",
    "    test_data = test_data.prefetch(1)\n",
    "    \n",
    "    iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                               train_data.output_shapes)\n",
    "    x, y = iterator.get_next()\n",
    "\n",
    "    train_init = iterator.make_initializer(train_data) # Inicializador para train_data\n",
    "    test_init = iterator.make_initializer(test_data) # Inicializador para test_data\n",
    "    \n",
    "    # Visualize input x\n",
    "    tf.summary.image(\"input\", x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    def conv2d(img, w, b):\n",
    "        return tf.nn.leaky_relu(tf.nn.bias_add\\\n",
    "            (tf.nn.conv2d(img, w,\\\n",
    "            strides=[1, 1, 1, 1],\\\n",
    "            padding='SAME'),b), alpha=alpha)\n",
    "\n",
    "    def max_pool(img, k):\n",
    "        return tf.nn.max_pool(img, \\\n",
    "            ksize=[1, k, k, 1],\\\n",
    "            strides=[1, k, k, 1],\\\n",
    "            padding='SAME')\n",
    "\n",
    "    wc1 = tf.Variable(tf.random_normal([3, 3, 1, 16]), name=\"wc1\")\n",
    "    bc1 = tf.Variable(tf.random_normal([16]), name=\"bc1\")\n",
    "    wc2 = tf.Variable(tf.random_normal([3, 3, 16, 32]), name=\"wc2\")\n",
    "    bc2 = tf.Variable(tf.random_normal([32]), name=\"bc2\")\n",
    "    # pool 128x128\n",
    "    wc3 = tf.Variable(tf.random_normal([1, 1, 32, 64]), name=\"wc3\")\n",
    "    bc3 = tf.Variable(tf.random_normal([64]), name=\"bc3\")\n",
    "    # pool 64x64\n",
    "    wc4 = tf.Variable(tf.random_normal([1, 1, 64, 128]), name=\"wc4\")\n",
    "    bc4 = tf.Variable(tf.random_normal([128]), name=\"bc4\")\n",
    "    # pool 32x32\n",
    "    wd1 = tf.Variable(tf.random_normal([32*32*128, 512]), name=\"wd1\")\n",
    "    bd1 = tf.Variable(tf.random_normal([512]), name=\"bd1\")\n",
    "    wd2 = tf.Variable(tf.random_normal([512, 1024]), name=\"wd2\")\n",
    "    bd2 = tf.Variable(tf.random_normal([1024]), name=\"bd2\")\n",
    "    wout = tf.Variable(tf.random_normal([1024, n_classes]), name=\"wout\")\n",
    "    bout = tf.Variable(tf.random_normal([n_classes]), name=\"bout\")\n",
    "\n",
    "    # conv layer\n",
    "    conv1 = conv2d(x,wc1,bc1)\n",
    "\n",
    "    # conv layer\n",
    "    conv2 = conv2d(conv1,wc2,bc2)\n",
    "\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 64*64 matrix.\n",
    "    conv2 = max_pool(conv2, k=2)\n",
    "    # conv2 = avg_pool(conv2, k=2)\n",
    "\n",
    "    # dropout to reduce overfitting\n",
    "    keep_prob = tf. placeholder(tf.float32)\n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "\n",
    "    # conv layer\n",
    "    conv3= conv2d(conv2,wc3,bc3)\n",
    "\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 64*64 matrix.\n",
    "    conv3 = max_pool(conv3, k=2)\n",
    "    # conv2 = avg_pool(conv2, k=2)\n",
    "\n",
    "    # dropout to reduce overfitting\n",
    "    conv3 = tf.nn.dropout(conv3, keep_prob)\n",
    "\n",
    "    # conv layer\n",
    "    conv4 = conv2d(conv3,wc4,bc4)\n",
    "\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 64*64 matrix.\n",
    "    conv4 = max_pool(conv4, k=2)\n",
    "    # conv2 = avg_pool(conv2, k=2)\n",
    "\n",
    "    # dropout to reduce overfitting\n",
    "    conv4 = tf.nn.dropout(conv4, keep_prob)\n",
    "\n",
    "    # fc 1\n",
    "    dense1 = tf.reshape(conv4, [-1, wd1.get_shape().as_list()[0]])\n",
    "    dense1 = tf.nn.leaky_relu(tf.add(tf.matmul(dense1, wd1),bd1), alpha=alpha)\n",
    "    dense1 = tf.nn.dropout(dense1, keep_prob)\n",
    "\n",
    "    # fc 2\n",
    "    dense2 = tf.reshape(dense1, [-1, wd2.get_shape().as_list()[0]])\n",
    "    dense2 = tf.nn.leaky_relu(tf.add(tf.matmul(dense2, wd2),bd2), alpha=alpha)\n",
    "    dense2 = tf.nn.dropout(dense2, keep_prob)\n",
    "\n",
    "    # prediction\n",
    "    pred = tf.add(tf.matmul(dense2, wout), bout)\n",
    "    weighted_pred = tf.multiply(pred, class_weight)\n",
    "\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        # softmax\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=weighted_pred, labels=y))\n",
    "        tf.summary.scalar(\"cross_entropy\", cost)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        # Accuracy\n",
    "        correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)    \n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    # Get all summary\n",
    "    summ = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()\n",
    "    config = tf.ConfigProto(allow_soft_placement = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session start\n",
    "with tf.Session(config=config) as sess:    \n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()\n",
    "    train_writer = tf.summary.FileWriter(summaries_dir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(summaries_dir + '/test', sess.graph)\n",
    "    \n",
    "    # Required to get the filename matching to run.\n",
    "    sess.run(init)\n",
    "    \n",
    "    total_epoch_time = 0\n",
    "    step = 1\n",
    "    # Compute epochs.\n",
    "    for i in range(training_epochs):\n",
    "        print(\"epoch: {}\\n\".format(i))\n",
    "        print(\"\\n\")\n",
    "        epoch_start = time.time()\n",
    "        sess.run(train_init)\n",
    "        epoch_step = 1\n",
    "        avg_acc = 0\n",
    "        avg_loss = 0\n",
    "        try:\n",
    "            while True:\n",
    "                _, acc, loss, summary_str = sess.run([optimizer, accuracy, cost, summ], feed_dict={keep_prob: dropout}, options=run_options,run_metadata = run_metadata) \n",
    "\n",
    "                train_writer.add_summary(summary_str, step)\n",
    "\n",
    "                if step % display_step == 0:\n",
    "                    train_writer.add_run_metadata(run_metadata,\"step {}\".format(step))\n",
    "                    print(\"step: {}\".format(step))\n",
    "                    print(\"accuracy: {}\".format(acc))\n",
    "                    print(\"loss: {}\".format(loss))\n",
    "                    print(\"\\n\")\n",
    "                avg_acc += acc\n",
    "                avg_loss += loss\n",
    "                step += 1\n",
    "                epoch_step += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            total_epoch_time += epoch_time\n",
    "            print(\"epoch finished in {} seconds\".format(epoch_time))\n",
    "            print(\"Average epoch accuracy is {:.2f}%\".format(epoch_step,(avg_acc / epoch_step) * 100))\n",
    "            print(\"Average epoch loss is {:.2f}\".format(epoch_step,(avg_loss / epoch_step)))\n",
    "\n",
    "            # Test            \n",
    "\n",
    "            print(\"Test\\n\")\n",
    "            sess.run(test_init)\n",
    "            avg_acc = 0\n",
    "            avg_loss = 0\n",
    "            test_step=0\n",
    "            try:\n",
    "                while True:\n",
    "                    acc, loss, summary_str = sess.run([accuracy, cost, summ], feed_dict={keep_prob: 1.})\n",
    "                    avg_acc += acc\n",
    "                    avg_loss += loss\n",
    "                    steps += 1\n",
    "\n",
    "                    test_writer.add_summary(summary_str, test_step)\n",
    "                    \n",
    "                    print(\"accuracy: {}\".format(acc))\n",
    "                    print(\"loss: {}\".format(loss))\n",
    "                    print(\"\\n\")\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(test_step,(avg_acc / test_step) * 100))\n",
    "                print(\"Average validation set loss over {} iterations is {:.2f}\".format(test_step,(avg_loss / test_step)))\n",
    "                print(\"\\n\")\n",
    "\n",
    "    print(\"Average epoch time: {} seconds\".format(total_epoch_time/training_epochs))\n",
    "    writer.add_run_metadata(run_metadata,\"mySess\")\n",
    "    train_writer.close()\n",
    "    test_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
